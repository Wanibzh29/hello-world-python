{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of characters in a word.\n",
    "# for instance abccba has nb_chars = 6\n",
    "nb_chars = 4\n",
    "\n",
    "# number of possible characters used during the encoding.\n",
    "# for instance abcde leads to 01234 has nb_letters = 5\n",
    "nb_letters = 10\n",
    "#nb_letters = 26\n",
    "\n",
    "# number of words samples to be generated for the training\n",
    "nb_words = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of combinations\n",
    "nb_letters**nb_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs(nb_words, nb_chars, nb_letters):\n",
    "    '''Create a numpy array of nb_words rows with nb_chars columns each element\n",
    "    being a random letter of nb_letters (a, b...)'''\n",
    "    words = np.zeros((nb_words, nb_chars), dtype=int)\n",
    "    for w in range(nb_words):\n",
    "        for c in range(nb_chars):\n",
    "            i = random.randint(0, nb_letters-1)\n",
    "            words[w, c] = ord('a') + i\n",
    "    return words\n",
    "\n",
    "\n",
    "def encrypt(words, nb_words, nb_chars):\n",
    "    '''Encrypt each element of a numpy array of nb_words rows with nb_chars \n",
    "    columns each item with a secret algorithm'''\n",
    "    \n",
    "    encrypted_words = words.copy()\n",
    "    encrypted_words_probs = np.zeros((nb_words, nb_chars, nb_chars))\n",
    "    \n",
    "    #val_max = -1\n",
    "    \n",
    "    for w in range(nb_words):\n",
    "        for c in range(nb_chars): # 0,1,2,3,4\n",
    "            encrypted_words[w,c] = int(words[w,c]) - 49\n",
    "            val = encrypted_words[w,c] - 48\n",
    "            \n",
    "            #if val > val_max:\n",
    "            #    val_max = val\n",
    "            \n",
    "            # add entropy (i.e. mistakes in the encryption)\n",
    "            #epsilon = random.randint(0, 100)\n",
    "            #if epsilon == 5 and val != val_max:\n",
    "            #val +=1\n",
    "            \n",
    "            #print('w:',w,', c:',c,', [wc]:', val)\n",
    "            #encrypted_words_probs[w, c, val ] = 1.0\n",
    "            encrypted_words[w,c] = val\n",
    "    return encrypted_words\n",
    "\n",
    "def print_output(words, nb_words, nb_chars):\n",
    "    for w in range(nb_words):\n",
    "        word = ''\n",
    "        for c in range(nb_chars):\n",
    "            word += chr(words[w,c])\n",
    "        print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_features = nb_chars\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = layers.Input(shape=(nb_chars,), dtype='float32', name='main_input')\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = layers.Dense(2048, activation='relu', name='hl_1')(inputs)\n",
    "#x = layers.Dense(128, activation='relu', name='hl_1')(inputs)\n",
    "#x = layers.Dense(64, activation='relu', name='hl_2')(x)\n",
    "\n",
    "outputs = []\n",
    "losses = {}\n",
    "for o in range(nb_chars):\n",
    "    name_i = 'output_'+str(o)\n",
    "    output_i = layers.Dense(nb_letters, activation='softmax', dtype='float32', name=name_i)(x)\n",
    "    outputs.append(output_i)\n",
    "    losses[name_i] = 'categorical_crossentropy'\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=losses,\n",
    "              metrics=['accuracy'])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hl_1 (Dense)                    (None, 2048)         10240       main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_0 (Dense)                (None, 10)           20490       hl_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dense)                (None, 10)           20490       hl_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "output_2 (Dense)                (None, 10)           20490       hl_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "output_3 (Dense)                (None, 10)           20490       hl_1[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 92,200\n",
      "Trainable params: 92,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_x(x):\n",
    "    words = []\n",
    "    for w in x:\n",
    "        word = ''\n",
    "        for c in w:\n",
    "            word += chr(c)\n",
    "        words.append(word)\n",
    "   \n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cife', 'bdaj', 'hgai']\n",
      "x:\n",
      " [[ 99 105 102 101]\n",
      " [ 98 100  97 106]\n",
      " [104 103  97 105]] out of  3000\n",
      "\n",
      "x_train:\n",
      " [[-0.83847761  1.22588888  0.17948421 -0.21828492]\n",
      " [-1.18291591 -0.53510836 -1.55970385  1.51688934]\n",
      " [ 0.88371384  0.52148998 -1.55970385  1.16985449]] out of  3000\n",
      "\n",
      "y:\n",
      " [[2 8 5 4]\n",
      " [1 3 0 9]\n",
      " [7 6 0 8]\n",
      " ...\n",
      " [3 4 5 0]\n",
      " [7 8 6 5]\n",
      " [1 6 1 0]]\n",
      "\n",
      "y_train:\n",
      " [[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]] out of  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# ALL IN ONE STEP ('OSSIFIED')\n",
    "\n",
    "x = create_inputs(nb_words, nb_chars, nb_letters)\n",
    "display_x(x[:3])\n",
    "print('x:\\n', x[:3], 'out of ',len(x))\n",
    "print()\n",
    "\n",
    "# process the x data as useful ANN input data\n",
    "scaler = StandardScaler()\n",
    "x_train  = scaler.fit_transform(x)\n",
    "\n",
    "print('x_train:\\n', x_train[:3], 'out of ',len(x_train))\n",
    "print()\n",
    "\n",
    "# create output data for training\n",
    "y = encrypt(x, nb_words, nb_chars)\n",
    "print('y:\\n', y)\n",
    "print()\n",
    "# process the y data as useful ANN output data\n",
    "y_train = keras.utils.to_categorical(y, nb_letters)\n",
    "print('y_train:\\n', y_train[:3], 'out of ',len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_sub_set(y, c_ref, nb_chars):\n",
    "    '''Retrieve the probalities of the i-th char'''\n",
    "    nb_samples = len(y)\n",
    "    \n",
    "    yi = np.zeros((nb_samples, nb_letters), dtype=int)\n",
    "    \n",
    "    for s in range(nb_samples):\n",
    "        for c in range(nb_chars):\n",
    "            #print('ysl:',y[s][0][l_i])\n",
    "            if c == c_ref:\n",
    "                yi[s] = y[s, c]\n",
    "                \n",
    "    return yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = []\n",
    "for c in range(nb_chars):\n",
    "    yi_train = get_sub_sub_set(y_train, c, nb_chars)\n",
    "    y_train2.append(yi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0, 0, 1, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 5s 2ms/step - loss: 4.9601 - output_0_loss: 1.2525 - output_1_loss: 1.2303 - output_2_loss: 1.2373 - output_3_loss: 1.2399 - output_0_acc: 0.4817 - output_1_acc: 0.5027 - output_2_acc: 0.4880 - output_3_acc: 0.5000\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 2.5073 - output_0_loss: 0.6481 - output_1_loss: 0.6301 - output_2_loss: 0.6027 - output_3_loss: 0.6264 - output_0_acc: 0.7530 - output_1_acc: 0.7677 - output_2_acc: 0.7753 - output_3_acc: 0.7717\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 1.6096 - output_0_loss: 0.4365 - output_1_loss: 0.4013 - output_2_loss: 0.3854 - output_3_loss: 0.3864 - output_0_acc: 0.8530 - output_1_acc: 0.8660 - output_2_acc: 0.8703 - output_3_acc: 0.8790\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 1.1461 - output_0_loss: 0.2886 - output_1_loss: 0.2800 - output_2_loss: 0.2964 - output_3_loss: 0.2810 - output_0_acc: 0.9070 - output_1_acc: 0.9040 - output_2_acc: 0.9040 - output_3_acc: 0.9157\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.8391 - output_0_loss: 0.2117 - output_1_loss: 0.2109 - output_2_loss: 0.2121 - output_3_loss: 0.2044 - output_0_acc: 0.9363 - output_1_acc: 0.9347 - output_2_acc: 0.9367 - output_3_acc: 0.9403\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.6433 - output_0_loss: 0.1612 - output_1_loss: 0.1547 - output_2_loss: 0.1630 - output_3_loss: 0.1644 - output_0_acc: 0.9560 - output_1_acc: 0.9580 - output_2_acc: 0.9567 - output_3_acc: 0.9537\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.5098 - output_0_loss: 0.1315 - output_1_loss: 0.1183 - output_2_loss: 0.1139 - output_3_loss: 0.1461 - output_0_acc: 0.9660 - output_1_acc: 0.9703 - output_2_acc: 0.9723 - output_3_acc: 0.9533\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.4141 - output_0_loss: 0.1113 - output_1_loss: 0.0896 - output_2_loss: 0.0990 - output_3_loss: 0.1143 - output_0_acc: 0.9700 - output_1_acc: 0.9810 - output_2_acc: 0.9740 - output_3_acc: 0.9700\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.3028 - output_0_loss: 0.0753 - output_1_loss: 0.0705 - output_2_loss: 0.0795 - output_3_loss: 0.0775 - output_0_acc: 0.9807 - output_1_acc: 0.9853 - output_2_acc: 0.9820 - output_3_acc: 0.9830\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 4s 1ms/step - loss: 0.2586 - output_0_loss: 0.0617 - output_1_loss: 0.0564 - output_2_loss: 0.0665 - output_3_loss: 0.0740 - output_0_acc: 0.9850 - output_1_acc: 0.9910 - output_2_acc: 0.9817 - output_3_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train2, epochs=10, batch_size=1,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_readable_prediction(prediction, nb_words):\n",
    "    \n",
    "    words = [''] * nb_words\n",
    "    \n",
    "    c_i = 0\n",
    "    for char_prediction in prediction:\n",
    "\n",
    "        s_i = 0\n",
    "        for sample_char_prediction in char_prediction:\n",
    "\n",
    "            i_letter = 0\n",
    "            best_value = -float('inf')\n",
    "            best_letter = -1\n",
    "            for letter_prediction in sample_char_prediction:\n",
    "                if letter_prediction > best_value:\n",
    "                    best_value = letter_prediction\n",
    "                    best_letter = i_letter\n",
    "                i_letter += 1\n",
    "            words[s_i] += str(best_letter)\n",
    "            words[s_i] += '-'\n",
    "            s_i += 1\n",
    "        c_i += 1\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jahi', 'fhad']\n",
      "x_test=\n",
      " [[106  97 104 105]\n",
      " [102 104  97 100]]\n",
      "x_test_mean=\n",
      " [[ 1.57259043 -1.59170671  0.87515943  1.16985449]\n",
      " [ 0.19483726  0.87368943 -1.55970385 -0.56531977]]\n",
      "-->\n",
      "prediction\n",
      "['9-0-8-9-', '5-7-0-3-']\n",
      "check prediction\n",
      "y_test=\n",
      " [[9 0 7 8]\n",
      " [5 7 0 3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "nb_words = 2\n",
    "\n",
    "x_test = create_inputs(nb_words, nb_chars, nb_letters)\n",
    "display_x(x_test)\n",
    "print(\"x_test=\\n\", x_test)\n",
    "\n",
    "x_test_n  = scaler.transform(x_test)\n",
    "print(\"x_test_mean=\\n\", x_test_n)\n",
    "print('-->')\n",
    "\n",
    "prediction = model.predict(x_test_n)\n",
    "#print(prediction)\n",
    "print('prediction')\n",
    "print_readable_prediction(prediction, nb_words)\n",
    "\n",
    "print('check prediction')\n",
    "y_test = encrypt(x_test, nb_words, nb_chars)\n",
    "print(\"y_test=\\n\", y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
